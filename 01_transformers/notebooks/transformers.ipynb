{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformer models are used to solve all kinds of NLP tasks.\n",
    "\n",
    "The HuggingFace transformers pipelines can do the following tasks:\n",
    "\n",
    "* feature-extraction\n",
    "* fill-mask\n",
    "* NER (named entity recognition)\n",
    "* question-answering\n",
    "* sentiment-analysis\n",
    "* summarization\n",
    "* text-generation\n",
    "* translation\n",
    "* zero-shot-classification\n",
    "\n",
    "There are many pre-trained models hosted in [HuggingFace](https://huggingface.co/models) that could be used for each of these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anaconda3/envs/asdrp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998623132705688}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTIMENT_MODEL = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "analyzer = pipeline(\"sentiment-analysis\", \n",
    "                    model=SENTIMENT_MODEL)\n",
    "analyzer(\"I am feeling pretty cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9995741248130798}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer(\"The weather is a bit dreary today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9689047932624817},\n",
       " {'label': 'POSITIVE', 'score': 0.9889335632324219},\n",
       " {'label': 'NEGATIVE', 'score': 0.9722331166267395}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer([\n",
    "    \"The US is in North American.\",\n",
    "    \"The teacher is a boy.\",\n",
    "    \"Earth is heating up dramtically.\"\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Classification\n",
    "\n",
    "Attempt to classify texts which were not pre-labelled.  This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!\n",
    "\n",
    "Source: [HF](https://huggingface.co/tasks/zero-shot-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/bart-large-mnli\"\n",
    "LABELS = [\"science\", \"food\", \"travel\", \"education\", \"politics\", \"business\"]\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=MODEL_NAME)\n",
    "statement = \"Joe Biden's presidential campaign fund has not increased much since last year.\"\n",
    "result = classifier(\n",
    "    statement,\n",
    "    candidate_labels=LABELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"Joe Biden's presidential campaign fund has not increased much since last year.\",\n",
       " 'labels': ['politics', 'business', 'travel', 'science', 'food', 'education'],\n",
       " 'scores': [0.9594570398330688,\n",
       "  0.013086460530757904,\n",
       "  0.012741496786475182,\n",
       "  0.007973656058311462,\n",
       "  0.003598777111619711,\n",
       "  0.0031426202040165663]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(query: str):\n",
    "    result = classifier(\n",
    "        query,\n",
    "        candidate_labels=LABELS\n",
    "    )\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Classification: \" + result['labels'][np.argmax(result['scores'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Joe Biden's presidential campaign fund has not increased much since last year.\n",
      "Classification: politics\n"
     ]
    }
   ],
   "source": [
    "classify(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Costa Rica has some amazing beaches for a summer vacation.\n",
      "Classification: travel\n"
     ]
    }
   ],
   "source": [
    "classify(\"Costa Rica has some amazing beaches for a summer vacation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Chocolate fondue is a nice snack for the winter.\n",
      "Classification: food\n"
     ]
    }
   ],
   "source": [
    "classify(\"Chocolate fondue is a nice snack for the winter.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    " You can find the list of selected open-source large language models (LLM) [here](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), ranked by their performance scores.\n",
    "\n",
    "Source: [HF](https://huggingface.co/tasks/text-generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 718/718 [00:00<00:00, 859kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.52G/1.52G [00:52<00:00, 28.8MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 149kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 1.04MB [00:00, 7.37MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 456kB [00:00, 7.45MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 1.36MB [00:00, 8.06MB/s]\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"distilgpt2\"\n",
    "# MODEL_NAME = \"bigscience/bloom-560m\"\n",
    "MODEL_NAME = \"gpt2-medium\"\n",
    "generator = pipeline(\"text-generation\", model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"This weekend, I plan to \\xa0join some friends over to Boston for some delicious dinner.\\xa0 We usually don't do as much shopping because we tend to try to avoid spending much money and trying to get everything done. This week, I'm more relaxed. I don't really have a lot of money either due to not having much left to spend.\\xa0 I don't want to try and\\xa0 make it all work to get things done if that means\\xa0 starting an entirely new blog. However, as I've\\xa0 gotten older it's become a lot easier to donen\\xa0 with nothing to spend and\\xa0 taking less money since it\"},\n",
       " {'generated_text': 'This weekend, I plan to \\xa0share my experiences, findings and solutions with you. The idea is to offer a bit of everything (including all of the previous ones).\\nPosted by\\xa0 J.J. at 7:58 AM'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"This weekend, I plan to \"\n",
    "responses = generator(prompt, \n",
    "                     max_length=128,\n",
    "                     num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "def printGeneratedText(str):\n",
    "    output = re.sub(r'\\s+', ' ', str)\n",
    "    pprint(f\"=> {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('=> This weekend, I plan to join some friends over to Boston for some '\n",
      " \"delicious dinner. We usually don't do as much shopping because we tend to \"\n",
      " 'try to avoid spending much money and trying to get everything done. This '\n",
      " \"week, I'm more relaxed. I don't really have a lot of money either due to not \"\n",
      " \"having much left to spend. I don't want to try and make it all work to get \"\n",
      " \"things done if that means starting an entirely new blog. However, as I've \"\n",
      " \"gotten older it's become a lot easier to donen with nothing to spend and \"\n",
      " 'taking less money since it')\n",
      "('=> This weekend, I plan to share my experiences, findings and solutions with '\n",
      " 'you. The idea is to offer a bit of everything (including all of the previous '\n",
      " 'ones). Posted by J.J. at 7:58 AM')\n"
     ]
    }
   ],
   "source": [
    "for r in responses:\n",
    "    printGeneratedText(r['generated_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Filling\n",
    "\n",
    "The idea of this task is to fill in the blanks in a given text.  The top_k argument controls how many possibilities you want to be displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilroberta-base\"\n",
    "unmasker = pipeline(\"fill-mask\",\n",
    "                    model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06936365365982056,\n",
       "  'token': 1307,\n",
       "  'token_str': ' huge',\n",
       "  'sequence': 'The building is huge and very expensive.'},\n",
       " {'score': 0.046456728130578995,\n",
       "  'token': 739,\n",
       "  'token_str': ' large',\n",
       "  'sequence': 'The building is large and very expensive.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The building is <mask> and very expensive.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3738466203212738,\n",
       "  'token': 37,\n",
       "  'token_str': ' he',\n",
       "  'sequence': 'Because this doctor is very capable, he can easily do this operation successfully.'},\n",
       " {'score': 0.1665724217891693,\n",
       "  'token': 25705,\n",
       "  'token_str': ' surgeons',\n",
       "  'sequence': 'Because this doctor is very capable, surgeons can easily do this operation successfully.'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Because this doctor is very capable, <mask> can easily do this operation successfully.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4550214409828186,\n",
       "  'token': 79,\n",
       "  'token_str': ' she',\n",
       "  'sequence': 'Because this nurse is very capable, she can easily take care of this patient.'},\n",
       " {'score': 0.09426577389240265,\n",
       "  'token': 52,\n",
       "  'token_str': ' we',\n",
       "  'sequence': 'Because this nurse is very capable, we can easily take care of this patient.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Because this nurse is very capable, <mask> can easily take care of this patient.\", top_k=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations.\n",
    "\n",
    "The option `grouped_entities=True` in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anaconda3/envs/asdrp/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "ner = pipeline(\"ner\", \n",
    "               model=MODEL_NAME,\n",
    "               grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9992048,\n",
       "  'word': 'James Cameron',\n",
       "  'start': 0,\n",
       "  'end': 13},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.99105597,\n",
       "  'word': 'Titanic',\n",
       "  'start': 46,\n",
       "  'end': 53},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.74788696,\n",
       "  'word': 'Titan',\n",
       "  'start': 135,\n",
       "  'end': 140}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"James Cameron, who directed the hit 1997 film Titanic and has made 33 dives to the wreckage, said he saw some similarities between the Titan tragedy and the sinking of the famous ship it was bound for.\"\n",
    "ner(statement)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question & Answering (QA)\n",
    "\n",
    "The question-answering pipeline answers questions using information from a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-cased-distilled-squad\"\n",
    "qa = pipeline(\"question-answering\",\n",
    "                             model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.09470159560441971,\n",
       " 'start': 98,\n",
       " 'end': 183,\n",
       " 'answer': 'he saw some similarities between the Titan tragedy and the sinking of the famous ship'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\n",
    "    question=\"What did James Cameron say?\",\n",
    "    context=statement\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"\"\"\n",
    "Catastrophic implosion: \n",
    "\n",
    "The Titanic-bound submersible that went missing on \n",
    "Sunday with five people on board suffered a \n",
    "“catastrophic implosion,” killing everyone on board, \n",
    "US Coast Guard Rear Adm. John Mauger said Thursday. \n",
    "A remotely operated vehicle found the tail cone of the \n",
    "Titan about 1,600 feet away from the bow of the shipwreck, \n",
    "Mauger said.\n",
    "\n",
    "Who was on board: Tour company OceanGate Expeditions \n",
    "said the five passengers were Hamish Harding, \n",
    "Shahzada Dawood and his son Suleman Dawood, \n",
    "Paul-Henri Nargeolet and OceanGate CEO Stockton Rush.\n",
    "\n",
    "About the trip: The Titan began its descent Sunday to \n",
    "explore the wreckage of the Titanic, located about \n",
    "13,000 feet below sea level in the North Atlantic Ocean.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sshleifer/distilbart-cnn-12-6\"\n",
    "summarizer = pipeline(\"summarization\",\n",
    "                      model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The tail cone of the Titanic-bound submersible was found about 1,600 feet away from the bow of the shipwreck . It suffered a “catastrophic implosion” killing everyone on board, US Coast Guard Rear Adm. John Mauger says .'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summarizer(news)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('=>  The tail cone of the Titanic-bound submersible was found about 1,600 '\n",
      " 'feet away from the bow of the shipwreck . It suffered a “catastrophic '\n",
      " 'implosion” killing everyone on board, US Coast Guard Rear Adm. John Mauger '\n",
      " 'says .')\n"
     ]
    }
   ],
   "source": [
    "printGeneratedText(summary[0]['summary_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translations\n",
    "\n",
    "For translation, you can use a default model if you provide a language pair in the task name (such as \"translation_en_to_fr\"), but the easiest way is to pick the model you want to use on the [Model Hub](https://huggingface.co/models). Here we’ll try translating from French to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 1.42kB [00:00, 4.40MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 301M/301M [00:10<00:00, 28.9MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 141kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 19.8kB/s]\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 802k/802k [00:00<00:00, 2.75MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 778k/778k [00:00<00:00, 3.23MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 1.34MB [00:00, 12.8MB/s]\n",
      "/Users/anaconda3/envs/asdrp/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "\n",
    "fr2en_translator = pipeline(\"translation\", \n",
    "                         model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'How are you?'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr2en_translator(\"comment allez-vous?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'I find this new field of artificial intelligence very interesting.'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr2en_translator(\"Je trouve ce nouveau domaine de l'intelligence artificielle très intéressant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 1.42kB [00:00, 3.18MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 301M/301M [00:10<00:00, 29.1MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 122kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 16.6kB/s]\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 778k/778k [00:00<00:00, 12.8MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 802k/802k [00:00<00:00, 5.45MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 1.34MB [00:00, 22.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "\n",
    "en2fr_translator = pipeline(\"translation\", \n",
    "                         model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Qu'est-ce que tu fais ce week-end ?\"}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2fr_translator(\"What are you doing this weekend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Le dernier film Transformer a été une merveille à regarder!'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2fr_translator(\"The latest Transformer movie was a marvel to watch!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Generation\n",
    "\n",
    "HuggingFace has a [StarChat Playground](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground).  The base model has 16B parameters and was pretrained on one trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks (all permissively licensed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# required to login to HF with tokens from: https://huggingface.co/settings/tokens\n",
    "#\n",
    "# > huggingface-cli login\n",
    "#\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"bigcode/starcoderplus\"\n",
    "device = \"cpu\" # \"cuda\" for GPU usage or \"cpu\" for CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: download ~ 60GB of model file -- will take a long time\n",
    "# Alternatively, run the hosted HF version at: https://huggingface.co/bigcode/starcoderplus\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asdrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
